---
public: false
type: article
title: "On Machines that Learn through Experience"
date: "2021-04-21T22:40:32.169Z"
description: Reinforcement Learning is a concept from psychology that can be embedded in machines to derive intelligent decision-making. A fundamental question is how should the machine adapt its behavior as a result of its experience, in the purpose of maximizing the cumulative reward.
---

"Since the late 1960s, many artificial intelligence researchers have presumed that there are no general principles to be discovered, that intelligence is instead due to the possession of a vast number of special-purpose tricks, procedures, and heuristics. It was sometimes said that if we could just get enough relevant facts into a machine, say one million, or one billion, then it would become intelligent." (Sutton, 1992). However, a deep insight into Natural Intelligence reveals otherwise — it is random interactions, not structured data, which provoke learning.

![]()

With very little knowledge and no understanding, organisms manage to solve incredibly complex puzzles under extreme uncertainty. From a swarm of ants that finds an optimal path, through an unborn chick that hatches its egg, to a human infant that learns how to crawl — all are remarkable achievements that have been made possible only through the process of trial and error.

The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, the consequences of actions, and what to do to achieve goals. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior.

>"Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence." 
> <div class="source">— Richard S. Sutton</div>

It is not by chance that the phenomenon of trial and error is considered to be the key idea behind the design of artificial intelligence systems, as it reveals how systems can obtain complex knowledge without having to analytically understand the problem involved. It is the purest form of learning, unbiased by any past assumptions, preprocessing, or imitation. It is a systematic mechanical method for decision-making that converges to optimality, with an accuracy level that increases as more trials are performed.

The trial and error model stands at the basis of any intelligence, turning nature into a vast vibrant lab of constant experimentation. Even the very process of Natural Selection is a trial and error algorithm, as it constantly experimenting with new forms of life that are put to the test of time and resources. Science itself is a trial and error process, as it eliminates proposed hypotheses by observation and contradiction.

>"The method of learning by trial and error — of learning from our mistakes — seems to be fundamentally the same whether it is practiced by lower or by higher animals, by chimpanzees or by men of science." 
> <div class="source">— Karl Popper</div>

It was the American psychologist Eduard Thorndike who has, based on observing the behavior of cats, formulated this mechanism as the law of effect. This principle states that any behavior that is followed by pleasant consequences is likely to be repeated, and any behavior followed by unpleasant consequences is likely to be stopped. When clueless organisms are encountered with extremely complex tasks, they tend to instinctively behave randomly, interacting with the environment and gathering experiences. Those experiences are formed into knowledge, which then can be used for the purpose of distinguishing valuable actions from unwanted ones. With time, random actions are replaced with those actions that are more profitable than others. Randomity fades away as it clears its place to life experience.

>"There is no reasoning, no process of inference or comparison; there is no thinking about things, no putting two and two together; there are no ideas — the animal does not think of the box or of the food or of the act he is to perform."
> <div class="source">— Edward Thorndike</div>

However, the trial and error process wouldn't be possible if it wasn't for one vital ingredient — Feedback. It is the environment of the learner that provides a continuous response, allowing the learner to estimate the goodness of each decision that it takes. In most cases, the learner is unaware of the problem it aims to solve, and most of the time cannot even grasp its complexity. Instead of solving the problem algorithmically, the learner is blindly guided towards the solution as it follows feedback signals, or rewards, which either encourage its behavior or disapprove of it. B.F. Skinner, arguably the second most famous psychologist in the twenty century, has described this phenomenon as Reinforcement Learning (RL) because it is based on this series of positive and negative reinforcements that ultimately shape the behavior of a learner. This phenomenon is especially noticeable in the training process of dogs — throughout its training process, the dog receives a treat upon presenting wanted behavior and no treat otherwise. Since treats are pleasant consequences, the dog is likely to only repeat wanted behavior. Put differently, the dog learns to optimize its actions for the purpose of maximizing its food supply. Ultimately, the dog achieves a complex goal that is defined by the external trainer, such as performing a new trick.

Skinner has shown that by using reinforcement learning, he has managed to make animals accomplish extraordinarily complicated tasks, such as teaching pigeons how to play ping-pong. Though this may seem like a silly task to teach a bird, the ping-pong experiment shows that reinforcement learning can be used to learn dynamic, goal-based behaviors. He even believed that this is how infants learn to communicate — as trial and error learners, they start by producing random sounds and only repeating those words which cause the most reaction by their parents. As part of his behaviorism theory, skinner has even suggested that the very concept of free will is an illusion — All human action, he instead believed, was the direct result of reinforcement.

However, the first encounter of reinforcement learning with computing was arguably made by Alan Turing. In 1948, his paper Intelligent Machinery imagined mechanical randomization systems paralleling human choice. Unconventionally, he saw that computers could be built with little knowledge but given the ability to learn as children do. The paper was not quite a revolution, but certainly the start of one.

>"I suggest that there should be two keys which can be manipulated by the schoolmaster, and which represent the ideas of pleasure and pain. At later stages in education the machine would recognize certain other conditions as desirable owing to their having been constantly associated in the past with pleasure, and likewise certain others undesirable." 
> <div class="source">— Alan Turing</div>

To attain the process of reinforcement learning in a machine, the machine is programmed to interact with its environment in a trial and error fashion — It starts by performing random actions, as it perceives the environment's reaction to each action that it does. Then, it is programmed to gradually prefer actions that produce a more positive reaction than other actions. Similar to the training process of dogs, the machine would receive rewards in the shape of numbers, and its sole purpose would be to maximize its total reward. In fact, this is the reason why AI and games are so inherently related — for AI, any problem can be seen as a game where the goal is to win the maximum points.

Rewarding mechanisms are the backbone of reinforcement learning. It is a way of telling the machine what is the final goal and what are the constraints, while the machine is responsible for discovering how to achieve that goal. The reward signal thus defines what are the good and bad events for the agent.

As an example, consider an AI agent that controls an autonomous taxi. As its trainers, we might give it a large positive reward for completing a task in time, and a negative reward for causing a traffic violation. Driving off a cliff should definitely cost a huge negative reward. Since time is important for passengers, we would punish the agent with a small negative reward for any second that it has not reached the pickup point yet. We might also give it a small negative reward for every mile, so it would eventually find the shortest possible path, saving fuel and time.

>"Reinforcement learning is learning what to do — how to map situations to actions — so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. "
> <div class="source">— Richard S. Sutton</div>

However, rewards are not the only signals that are supplied to the machine. Biological systems can perceive, alongside rewards, the current state of the system, or at least part of it. Informally, we can think of the state as a signal conveying to the agent some sense of "how the environment is" at a particular time. Therefore, the machine would receive, upon each action that it takes, two distinguished signals — A matching reward and the new state of the environment.

More abstractly, the process of reinforcement learning can be seen as a pair of two entities: The agent, which is the learning entity, and the environment it interacts with, and which is designed by the trainer. The environment can be conceptually mapped into different states. The transition between states is done solely by the agent as it performs certain actions in the environment. To constitute a feedback loop between the agent and the environment, the agent, upon acting in a certain state, perceives two distinguished inputs from the environment, namely, the environment's new state and the reward given by performing that action in that state. The agent's single objective is to maximize the received total reward. A particular behavior of the agent in the environment is called a policy and is formally defined as a mapping from states to probabilities of selecting each possible action. This process can be mathematically represented as a Markov Decision Process (MDP).

>"MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made."
> <div class="source">— Richard S. Sutton</div>

The MDP framework is an abstraction of the problem of goal-directed learning from interaction. It proposes that any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to define the agent's goal (the rewards). This framework, in its general form, is intended to reflect essential features of reinforcement learning as they appear in Nature and real-world problems. For example, some states can be defined as terminal, in the sense that it is not possible to perform any actions from those states, and once being reached, the agent is being initialized in a predetermined starting state. This concept is useful to reflect episodic tasks, in which the agent–environment interaction naturally breaks down into a sequence of separate episodes, such as plays of a game, trips through a maze, or any sort of repeated interaction. Other important features that can be reflected by MDP are uncertainty and nondeterminism — in its general form, an MDP is a stochastic process, meaning that performing an action in a state might lead to a transition to different possible states and different possible rewards. The MDP framework can be visualized as a weighted directed graph, where the white vertexes represent the states, the black vertexes represent state-action pairs, the edges are the transitions and the weights are the rewards.

With the above representation, the agent can be seemed as discretely traverses the graph by performing actions in states while collecting encountered rewards. At any given timestamp t, the total reward gained by the agent, often called Return, can be described as follows:

$$
G_t = R_{t+1}+\gamma R_{t+2} + ...= \sum_{k=0}^{\infty}\gamma ^kR^{t+k+1} \;\;\;\;\;\;\; (1)
$$

where $0 ≤ γ ≤ 1$ is the discount rate. This parameter determines the present value of future rewards — when $γ$ is significantly smaller than 1, the importance of near-feature rewards is increased, making the agent greedy, whereas, when $γ$ approaches 1, the agent becomes farsighted.

With a mathematical representation for the reinforcement learning process, it is easier to address the main question in implementing reinforcement learning: How should the agent learn? How can the agent use its past experiences with the environment to enhance its behavior, in a way that would maximize the cumulative reward? Any answer to this question is a reinforcement learning algorithm that specifies how the agent's policy is changed as a result of its experience.

>**A fundamental question in implementing Reinforcement Learning in Machines is how should the machine adapt its behavior as a result of its experience, in the purpose of maximizing the cumulative reward?**
