---
public: true
type: article
title: "On Machines that Learn through Experience"
date: "2021-04-21T22:40:32.169Z"
description: Reinforcement Learning is a concept from psychology that can be embedded in machines to derive intelligent decision-making. A fundamental question is how should the machine adapt its behavior as a result of its experience, in the purpose of maximizing the cumulative reward.


---

<div class="preface">
"Since the late 1960s, many artificial intelligence researchers have presumed that there are no general principles to be discovered, that intelligence is instead due to the possession of a vast number of special-purpose tricks, procedures, and heuristics. It was sometimes said that if we could just get enough relevant facts into a machine, say one million, or one billion, then it would become intelligent." (Sutton, 1992). However, a deep insight into Natural Intelligence reveals otherwise — it is random interactions, not structured data, which provoke learning.
</div>

With very little knowledge and no understanding, organisms manage to solve incredibly complex puzzles under extreme uncertainty. From a swarm of ants that finds an optimal path, through an unborn chick that hatches its egg, to a human infant that learns how to crawl — all are remarkable achievements that have been made possible only through the process of trial and error.

The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, the consequences of actions, and what to do to achieve goals. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior.

>"Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence." 
> <div class="source">— Richard S. Sutton</div>

It is not by chance that the phenomenon of trial and error is considered to be the key idea behind the design of artificial intelligence systems, as it reveals how systems can obtain complex knowledge without having to analytically understand the problem involved. It is the purest form of learning, unbiased by any past assumptions, preprocessing, or imitation. It is a systematic mechanical method for decision-making that converges to optimality, with an accuracy level that increases as more trials are performed.

The trial and error model stands at the basis of any intelligence, turning nature into a vast vibrant lab of constant experimentation. Even the very process of Natural Selection is a trial and error algorithm, as it constantly experimenting with new forms of life that are put to the test of time and resources. Science itself is a trial and error process, as it eliminates proposed hypotheses by observation and contradiction.

>"The method of learning by trial and error — of learning from our mistakes — seems to be fundamentally the same whether it is practiced by lower or by higher animals, by chimpanzees or by men of science." 
> <div class="source">— Karl Popper</div>

It was the American psychologist Eduard Thorndike who has, based on observing the behavior of cats, formulated this mechanism as the law of effect. This principle states that any behavior that is followed by pleasant consequences is likely to be repeated, and any behavior followed by unpleasant consequences is likely to be stopped. When clueless organisms are encountered with extremely complex tasks, they tend to instinctively behave randomly, interacting with the environment and gathering experiences. Those experiences are formed into knowledge, which then can be used for the purpose of distinguishing valuable actions from unwanted ones. With time, random actions are replaced with those actions that are more profitable than others. Randomity fades away as it clears its place to life experience.

>"There is no reasoning, no process of inference or comparison; there is no thinking about things, no putting two and two together; there are no ideas — the animal does not think of the box or of the food or of the act he is to perform."
> <div class="source">— Edward Thorndike</div>

However, the trial and error process wouldn't be possible if it wasn't for one vital ingredient — Feedback. It is the environment of the learner that provides a continuous response, allowing the learner to estimate the goodness of each decision that it takes. In most cases, the learner is unaware of the problem it aims to solve, and most of the time cannot even grasp its complexity. Instead of solving the problem algorithmically, the learner is blindly guided towards the solution as it follows feedback signals, or rewards, which either encourage its behavior or disapprove of it. B.F. Skinner, arguably the second most famous psychologist in the twenty century, has described this phenomenon as Reinforcement Learning (RL) because it is based on this series of positive and negative reinforcements that ultimately shape the behavior of a learner. This phenomenon is especially noticeable in the training process of dogs — throughout its training process, the dog receives a treat upon presenting wanted behavior and no treat otherwise. Since treats are pleasant consequences, the dog is likely to only repeat wanted behavior. Put differently, the dog learns to optimize its actions for the purpose of maximizing its food supply. Ultimately, the dog achieves a complex goal that is defined by the external trainer, such as performing a new trick.

Skinner has shown that by using reinforcement learning, he has managed to make animals accomplish extraordinarily complicated tasks, such as teaching pigeons how to play ping-pong. Though this may seem like a silly task to teach a bird, the ping-pong experiment shows that reinforcement learning can be used to learn dynamic, goal-based behaviors. He even believed that this is how infants learn to communicate — as trial and error learners, they start by producing random sounds and only repeating those words which cause the most reaction by their parents. As part of his behaviorism theory, skinner has even suggested that the very concept of free will is an illusion — All human action, he instead believed, was the direct result of reinforcement.

However, the first encounter of reinforcement learning with computing was arguably made by Alan Turing. In 1948, his paper Intelligent Machinery imagined mechanical randomization systems paralleling human choice. Unconventionally, he saw that computers could be built with little knowledge but given the ability to learn as children do. The paper was not quite a revolution, but certainly the start of one.

>"I suggest that there should be two keys which can be manipulated by the schoolmaster, and which represent the ideas of pleasure and pain. At later stages in education the machine would recognize certain other conditions as desirable owing to their having been constantly associated in the past with pleasure, and likewise certain others undesirable." 
> <div class="source">— Alan Turing</div>

To attain the process of reinforcement learning in a machine, the machine is programmed to interact with its environment in a trial and error fashion — It starts by performing random actions, as it perceives the environment's reaction to each action that it does. Then, it is programmed to gradually prefer actions that produce a more positive reaction than other actions. Similar to the training process of dogs, the machine would receive rewards in the shape of numbers, and its sole purpose would be to maximize its total reward. In fact, this is the reason why AI and games are so inherently related — for AI, any problem can be seen as a game where the goal is to win the maximum points.

Rewarding mechanisms are the backbone of reinforcement learning. It is a way of telling the machine what is the final goal and what are the constraints, while the machine is responsible for discovering how to achieve that goal. The reward signal thus defines what are the good and bad events for the agent.

As an example, consider an AI agent that controls an autonomous taxi. As its trainers, we might give it a large positive reward for completing a task in time, and a negative reward for causing a traffic violation. Driving off a cliff should definitely cost a huge negative reward. Since time is important for passengers, we would punish the agent with a small negative reward for any second that it has not reached the pickup point yet. We might also give it a small negative reward for every mile, so it would eventually find the shortest possible path, saving fuel and time.

>"Reinforcement learning is learning what to do — how to map situations to actions — so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. "
> <div class="source">— Richard S. Sutton</div>

However, rewards are not the only signals that are supplied to the machine. Biological systems can perceive, alongside rewards, the current state of the system, or at least part of it. Informally, we can think of the state as a signal conveying to the agent some sense of "how the environment is" at a particular time. Therefore, the machine would receive, upon each action that it takes, two distinguished signals — A matching reward and the new state of the environment.

More abstractly, the process of reinforcement learning can be seen as a pair of two entities: The agent, which is the learning entity, and the environment it interacts with, and which is designed by the trainer. The environment can be conceptually mapped into different states. The transition between states is done solely by the agent as it performs certain actions in the environment. To constitute a feedback loop between the agent and the environment, the agent, upon acting in a certain state, perceives two distinguished inputs from the environment, namely, the environment's new state and the reward given by performing that action in that state. The agent's single objective is to maximize the received total reward. A particular behavior of the agent in the environment is called a policy and is formally defined as a mapping from states to probabilities of selecting each possible action. This process can be mathematically represented as a Markov Decision Process (MDP).

>"MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made."
> <div class="source">— Richard S. Sutton</div>

The MDP framework is an abstraction of the problem of goal-directed learning from interaction. It proposes that any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to define the agent's goal (the rewards). This framework, in its general form, is intended to reflect essential features of reinforcement learning as they appear in Nature and real-world problems. For example, some states can be defined as terminal, in the sense that it is not possible to perform any actions from those states, and once being reached, the agent is being initialized in a predetermined starting state. This concept is useful to reflect episodic tasks, in which the agent–environment interaction naturally breaks down into a sequence of separate episodes, such as plays of a game, trips through a maze, or any sort of repeated interaction. Other important features that can be reflected by MDP are uncertainty and nondeterminism — in its general form, an MDP is a stochastic process, meaning that performing an action in a state might lead to a transition to different possible states and different possible rewards. The MDP framework can be visualized as a weighted directed graph, where the white vertexes represent the states, the black vertexes represent state-action pairs, the edges are the transitions and the weights are the rewards.

With the above representation, the agent can be seemed as discretely traverses the graph by performing actions in states while collecting encountered rewards. At any given timestamp t, the total reward gained by the agent, often called Return, can be described as follows:

$$
G_t = R_{t+1}+\gamma R_{t+2} + ...= \sum_{k=0}^{\infty}\gamma ^kR^{t+k+1} \;\;\;\;\;\;\; (1)
$$

where $0 ≤ γ ≤ 1$ is the discount rate. This parameter determines the present value of future rewards — when $γ$ is significantly smaller than 1, the importance of near-feature rewards is increased, making the agent greedy, whereas, when $γ$ approaches 1, the agent becomes farsighted.

With a mathematical representation for the reinforcement learning process, it is easier to address the main question in implementing reinforcement learning: How should the agent learn? How can the agent use its past experiences with the environment to enhance its behavior, in a way that would maximize the cumulative reward? Any answer to this question is a reinforcement learning algorithm that specifies how the agent's policy is changed as a result of its experience.

>**A fundamental question in implementing Reinforcement Learning in Machines is how should the machine adapt its behavior as a result of its experience, in the purpose of maximizing the cumulative reward?**

##### Solution methods

A fundamental concept in most reinforcement learning algorithms is the idea of the value function. Whereas the reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the value of a state, with respect to a certain policy, is the total amount of reward an agent can expect to accumulate over the future, starting from that state and following the policy thereafter. Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states. For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards.

>"To make a human analogy, rewards are somewhat like pleasure (if high) and pain (if low), whereas values correspond to a more refined and farsighted judgment of how pleased or displeased we are that our environment is in a particular state." 
> <div class="source">— Richard S. Sutton</div>

Formally, given a policy $\pi$, the value of state $s$ in respect to $\pi$ is defined as follows:

$$
v_{\pi}(s)\doteq\mathbb{E}_{\pi}\left[G_{t}\mathrel{{\mid}{S_t=t}} \right] \;\;\;\;\;\;\; (2)
$$

where $G$, is defined by (1) and $\mathbb{E}$, with respect to $\pi$ is the *probabilistic expectation* of $G$ under policy $\pi$ (considering a possibly stochastic environment). Values can also be assigned to a *state-action pair*, not only to states. The value of a state-action pair $(s,a)$ under a certain policy $\pi$ is the expected return for starting at that state, performing that action, and following that policy thereafter, or formally:

$$
q_{\pi}(s,a)=\mathbb{E_\pi}[G_{t} |  S_t=s, A_t=a]\;\;\;\;\;\;\; (3)
$$

Most of the reinforcement learning algorithms are structured around estimating value functions (Other approaches include *Evolutionary algorithms* or *Policy Gradient Methods*, which are not discussed in this article). Once the value function has been estimated, the agent can use it to act in the environment in a near-optimal way — Starting from any state, the agent can use the state-value function to greedily select the action that leads to the state with the highest value, or, alternatively, to use the action-value function to greedily select the action that has the highest value.

In principle, there are numerous ways of computing the value function from a given MDP, such as algebraically resolving a system of nonlinear equations which express the MDP, or by using Dynamic Programming methods such as *Value Iteration* or *Policy Iteration*. Such approaches apply mathematical manipulation on the MDP components (states, actions, transitions, and rewards) to extract optimal solutions, with no interaction between the agent and the environment, under the assumption that the full description of the MDP is accessible and is fully known to the agent. However, those analytic approaches are not suitable for most cases where reinforcement learning is applied, since they require a full model of the environment and do not imitate the trial and error learning model. Therefore, we would be interested in solution methods that follow the trial and error paradigm. Such methods assume that the full model of the environment is not given (often known as model-free) and that only by interacting with the environment, the agent can gradually estimate the value function.

A basic example of such an algorithm is the *Monte Carlo method*. Roughly speaking, the algorithm estimates the value of each state-action pair by averaging the returns observed after performing the action in the state. As more returns are observed, the average should converge to the expected value. The algorithm starts with an arbitrarily selected policy $\pi$ and from an arbitrarily selected state, performs an arbitrary action, and traverses the MDP following $\pi$ until reaches a terminal state. Such traversal is sufficient to observe, for each state-action pair $(s,a)$ that was encountered, the return (the total reward) that was gained starting of that pair. The algorithm uses this observation to modify the action value — it averages the observed return with the current value of $(s,a)$. To align the policy with the updated value function, the algorithm modifies the policy so it would greedily follow the value function (meaning, choosing to perform actions that has the highest value). The algorithm continues by generating a new episode, now under the improved policy, which, in turn, derives a more accurate value estimation and so on. In this process, both the policy and the value function converge to their optimal values, until sufficient accuracy is reached, or when no more changes occur to the policy and the value function (the state of equilibrium).

This tactic, where two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement) is known as general policy iteration (GPI) and is not exclusive for Monte Carlo methods. In fact, almost all reinforcement learning methods are well described as GPI.

Another important factor in the Monte Carlo algorithm is that a new starting point is being randomly selected at the beginning of each episode. If it wasn't the case, and the policy was deterministic, then the agent would have never encountered most state-action pairs. This is the general problem of maintaining exploration, where algorithms that do not perform enough explorative moves would endure an extremely slow convergence rate, and, if the system or the policy are deterministic, they would only be able to reach solutions that are locally optimal, possibly missing-out much better solutions. To obtain a lot of rewards, a reinforcement learning agent must prefer actions that it has tried in the past and found to be effective in producing reward. But to discover such actions, it has to try actions that it has not selected before. The agent has to exploit what it has already experienced to obtain reward, but it also has to explore to make better action selections in the future. This is known as the exploration-exploitation dilemma.

>"The dilemma is that neither exploration nor exploitation can be pursued exclusively without failing at the task."
> <div class="source">— Richard S. Sutton</div>

To address this limitation, it is necessary to add a portion of randomity (noise) to the movement of the agent. Here, such randomity is added by alternating the starting point of the search on each episode. This guarantees that all state-action pairs will be visited an infinite number of times within the limit of an infinite number of episodes.

An obvious drawback of the Monte Carlo algorithm is that it relies on the existence of terminal states, which means that it is not equipped to address tasks that are not episodic. The Temporal-difference (TD) approach is a different method of estimating the value function which does not assume the existence of terminal states, meaning that it can handle tasks that are continuous in nature. Roughly speaking, unlike with the Monte Carlo method, a Temporal-difference algorithm does not waits until an end of an episode in order to adjust the values of encountered state-action pairs. Instead, it modifies those values locally, immediately after each step. More specifically, the agent traverses the MDP, sample encountered values, and gradually shifts those values towards a predetermined target value. This process is based on the following update rule, which characterizes TD-learning:

$$
\mathbf{New} \leftarrow \mathbf{Old} + \mathbf{Learning Rate} \cdot \overbrace{[\mathbf{Target} - \mathbf{Old}] }^{\mathbf{Error}} \;\; (4)
$$

where OldEstimate is the sampled value of a state-action pair, Target is the expected value, and LearningRate is the step-size. One example of a TD algorithm is State-action-reward-state-action (Sarsa). Whereas the Monte Carlo approach averages the values based on empirically measured returns, the Sarsa algorithm makes estimations based on a known mathematical relation that exists between values of state-action pairs in an MDP. This relation states that the expected value of a state-action pair, under a certain policy, is the sum of the immediate reward gained by performing the action and the (discounted) value of the next state-action pair. Formally,

$$
q_{\pi}(s,a)=\mathbb{E_\pi} [R_{t+1}+\gamma q_{\pi}(s_{t+1},a_{t+1}) |  S_t=s, A_t=a] \;\;\;\;\;\;\; (5)
$$

The above equation is known as *Bellman Expectation Equation*. Intuitively, it states that upon performing an action, the agent is expected to receive the immediate reward for performing that action, and the rest of the return is expected to be, by definition, the value of the next state-action pair. This relation expresses the *recursive nature* of values in an MDP. Unlike with relation (3) ,which describes the values of state-actions pairs in terms of global return, relation (5) describes those values using immediately computable terms - the immediate reward gained by performing the action, and the value of the next state-action pair which is determined by the policy. For this reason, relation (5) serves as a basis for the Sarsa algorithm, which relies on local online modifications to the value function, rather than on long-term computations. The Sarsa algorithm uses relation (5) as the target value of each state-action pair. Formally,

$$
q_{\pi'}(s_{t},a_{t}) \leftarrow q_{\pi}(s_{t},a_{t}) + \alpha  \overbrace{[\overbrace {R(s_{t},a_{t},s_{t+1})+\gamma q_{\pi}(s_{t+1},a_{t+1})}^{Target} - q_{\pi}(s_{t},a_{t})] }^{\mathbf{Error}}  \;\;\;\;\;\;\; (6)
$$

where $0 ≤ \alpha ≤ 1$ is the *learning rate*. When $\alpha$ approaches 1, the agent becomes hasty, as it immediately replaces old knowledge with new. Such an agent would not perform well in stochastic environments where samples may drastically vary. On the other hand, a very low learning rate would make an agent a slow learner. This is yet another trade-off that should be fine-tuned, perhaps empirically.

Similar to the Monte Carlo algorithm, the Sarsa algorithm uses a GPI approach to simultaneously modify the action-value function according to its policy and to modify its policy to greedily match the action-value function.

To maintain exploration, the algorithm does not purely follow the greedy path defined by action values. Instead, it occasionally performs a random move. Such behavior is known as the ε-greedy policy. As a result, the Sarsa algorithm does not converge to the absolute optimal policy, but to a near-optimal policy that is subject to occasional random actions. Put differently, this algorithm learns the ε-greedy policy, not the greedy policy. Indeed, adding random moves to the search process has downsides because a portion of randomity becomes a part of the result. To address this issue, the value of ε can be gradually decreased during learning, perhaps exponentially, which will ensure convergence to the absolute greedy policy. Another, more sophisticated way of addressing this issue is to allow the agent to estimate values according to a different policy than the one it follows. Those kinds of methods are known as off-policy algorithms, such as the Q-Learning algorithm, which will not be discussed here.

TD-learning methods are strikingly similar to the natural process of learning under uncertainty. For example, the <I>reward prediction error hypothesis of dopamine neuron activity</I> proposes that one of the functions of the phasic activity of dopamine-producing neurons in mammals is to deliver an error between an old and a new estimate of expected future reward to specific areas throughout the brain. This phenomenon was especially noticeable in a series of experiments conducted by Wolfram Schultz. In his experiments, a monkey was exposed to certain stimuli (tone or light), which was then followed by fruit juice, training the monkey to associate the stimulus with the reward of juice. By measuring the activity level of dopamine-producing neurons in the monkey's brain, Schultz has noticed that dopamine activity is increasing when fruit juice is supplied, indicating a difference in expected and actual rewards. Over time, dopamine activity towards the fruit juice was decreasing (meaning that fruit juice becomes expected), and instead, dopamine activity happened as a response to the stimuli itself. This means that the monkey has <I>learned</I> that fruit juice is followed by stimuli. Finally, when fruit juice is suddenly not supplied immediately after the appearance of stimuli, the dopamine activity level drops below baseline (indicating a negative difference between expected and actual rewards, a disappointment). This example emphasizes the basic idea of TD-learning — Learning is the process of narrowing down the gap between predictions and reality  ■